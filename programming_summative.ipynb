{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7aee48-5c93-40c9-9eed-83a99d5729f9",
   "metadata": {},
   "source": [
    "For Question 1, you are asked to perform the following tasks based on the following target website, which contains artificial content designed \n",
    "for this assignment: https://sitescrape.awh.durham.ac.uk/comp42315/ \n",
    "\n",
    "a) Please design and implement a solution to crawl the publication title, year and author list of every unique publication record on the target website. [10/35]\n",
    "\n",
    "b) This information should then be stored and displayed in an appropriate format. Displayed to the user should be: publication title, year, \n",
    "#author list, number of authors and impact factor.\n",
    "\n",
    "c) The records should be manipulatable by the user, at minimum you should be able to display sorted information according to: \n",
    "#descending year values, descending number of author values, the titles from A to Z, and finally by the impact of the papers. \n",
    "\n",
    "d) Explain your design and highlight any features in this question’s report part of your Jupyter Notebook in no more than 300 words. \n",
    "Reflect upon the importance of well structured HTML. [5/35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44f29069-9c18-4e86-bf87-7e60b1a9e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, HBox\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import textstat\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pgmpy as pg\n",
    "import tabulate as tb\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "from pgmpy.metrics.metrics import correlation_score, log_likelihood_score, structure_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1087a7-dd98-42cd-ab92-4f2a64e139da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question (a)\n",
    "page = requests.get(\"https://sitescrape.awh.durham.ac.uk/comp42315/publicationfull_year_characteranimation.htm\")\n",
    "page2 = page.content\n",
    "soup = bs(page2, \"html.parser\")\n",
    "divRows = soup.find_all(\"div\", class_=\"w3-container w3-cell w3-mobile w3-cell-middle\")\n",
    "texts = []\n",
    "for div in divRows:\n",
    "    texts.append(div.text.strip())\n",
    "\n",
    "#Question (b)\n",
    "publication_titles = []\n",
    "publisher = []\n",
    "years = []\n",
    "authors = []\n",
    "impact_factor = []\n",
    "for text in texts:\n",
    "    content = text.split(\"\\n\")\n",
    "    publication_titles.append(content[0])\n",
    "    #publisher.append(content[2][0:-6])\n",
    "    years.append(re.findall(r'\\d{4}', content[2])[-1])\n",
    "    authors.append(content[3])\n",
    "    if content[4].find(\"Impact\") == -1:\n",
    "        impact_factor.append(\"\")\n",
    "    else:\n",
    "        impact_factor.append(content[4][-6:-1])\n",
    "#print(publisher)\n",
    "\n",
    "dict = {'Publication Title': publication_titles, 'Year Published': years, 'Authors': authors, \"Impact Factor\": impact_factor} #\"Publisher\": publisher#} \n",
    "df1 = pd.DataFrame(dict)\n",
    "authors = df1[\"Authors\"].values.tolist()  \n",
    "A = []\n",
    "for author in authors:\n",
    "    normalized_authors = author.replace(\" and \", \", \")  \n",
    "    author_list = normalized_authors.split(\", \")  \n",
    "    A.append(len(author_list))\n",
    "                \n",
    "df1[\"Author Count\"] = A\n",
    "display(df1)\n",
    "\n",
    "#question (c)\n",
    "items = ['All'] + [col for col in df1.columns if col != 'Authors']\n",
    "a_slider = widgets.IntSlider(min=1, max=len(df1), step=1, value=5, description=\"Rows:\")\n",
    "b_select = widgets.Select(options=items, description=\"Column:\")\n",
    "\n",
    "def view2(x='All', y=3):\n",
    "    if x == 'All': \n",
    "        display(df1.head(y))  \n",
    "    if x == \"Publication Title\":\n",
    "        display(df1.sort_values(by=x, ascending=True).head(y))\n",
    "    if x == \"Year Published\":\n",
    "        display(df1.sort_values(by=x, ascending=False).head(y))\n",
    "    if x == \"Author Count\":\n",
    "        display(df1.sort_values(by=x, ascending=False).head(y))\n",
    "    if x == \"Impact Factor\":\n",
    "        display(df1.sort_values(by=x, ascending=False).head(y))\n",
    "        \n",
    "interactive_output = widgets.interactive_output(view2, {'x': b_select, 'y': a_slider})\n",
    "ui = VBox([HBox([b_select, a_slider]), interactive_output])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1841f3-8917-478f-92bc-86c98f17ebbd",
   "metadata": {},
   "source": [
    "The code above is divided into 3 parts. The first involves extracting the html code from the url page and parsing its content. This allows for the relevant information on each publication to be extracted.\r\n",
    "\r\n",
    "The second part is centred around extraction the information required from the data scraped. The year of publication is found using re.findall(r'\\d{4}') which finds 4 digit numbers. Because the year published is contained in the same string as the publication source which occasionally contains years in its title, the use of [-1] ensures it is only the last 4 digit number which is appended. The string containing the impact factor is in the format: \"impact facotr x.xxx#\", hence why the function appends the 5 characters prior to the last one in the string. Because of the “.” in the impact factor the use of re.findall(r'\\d{4}') is not possible. Because the number of authors is not explicitly stated on the website crawled, the use of a for loop and string manipulation is required. Finally, there is the possibility of including a column with the name of the journal or book the article was published in (simply remove the # in the for loop). The data is displayed using a pandas dataframe.\r\n",
    "\r\n",
    "Finally, interactive widgets are added which display the information in the dataframe depending on which column the user wishes to engage with. When one of the columns is selected it calls a function which arranges the items in that list in a previously determined order. The output provided by the widgets arranges the rows in alphabetical order for publication title, from newest to oldest for year published, from biggest to smallest impact factor, and from the greatest number of authors to the fewest for author count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc33bc-221a-44f2-973b-6dabc932a5ca",
   "metadata": {},
   "source": [
    "Question 2 (30 marks)\n",
    "\n",
    "For this question, you will analyse an existing corpus of privacy policies, following the paper https://arxiv.org/abs/2201.08739. You need to download policy-metadata.zip (25.4 MB) and policy-texts.zip (375.1 MB) from https://zenodo.org/records/7426577. You do not need to download any of the labelled datasets or policies, only the raw text and the metadata. The datasets are also provided in the submission folder and uploaded to the Blackboard ULTRA This question consists of 3 parts. [30%]\r\n",
    "\r\n",
    "a) For each policy text file, you need to compute the Flesch Reading Ease score, the SMOG index, the Coleman-Liau index, the Flesch-Kincaid Grade and the Dale-Chall readability score. After manually identifying which of those metrics are given in policy-metadata.csv, you need to compare the metrics you’ve obtained against those provided, identifying any difference. If you have found any difference, reflect on the possible reasons. [10/30]\r\n",
    "\r\n",
    "b) For each policy text file, you need to compute a sentiment analysis, including polarity and subjectivity, and present an overall analysis of the corpus, including a box plot for each metric, as well as as analysis of the median of each metric over the years. [10/30]\r\n",
    "\r\n",
    "c) Finally, you need to do a correlation analysis, assessing if any of the readability metrics in (a) are correlated to the sentiment analysis metrics in (b). [10/30]\r\n",
    "\r\n",
    "[Explain your design and highlight any features in this question’s report part of your Jupyter Notebook in no more than 300 words.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f647b-65de-4d54-b5c3-2a34c71adfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"/home3/COMP42315/policy-texts/policies_fromseg_custom/\")\n",
    "print(len(files))\n",
    "print(f\"Number of files: {len(files)}\")\n",
    "print(files[0:1])\n",
    "test = open(\"/home3/COMP42315/policy-texts/policies_fromseg_custom/721ebea55e45486b0fe2e7067d8b3097a588b1f6ad6a442ccc31af7b.txt\");\n",
    "\n",
    "file_contents = test.read()\n",
    "print(file_contents[0:100]);\n",
    "test.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49da3d-1fa8-4f57-86b2-c28f4b46eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"/home3/COMP42315/policy-metadata/policy-metadata.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d040216-bc26-49b8-8f30-a7a1d7983a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home3/COMP42315/policy-texts/policies_fromseg_custom/\"\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "readability_scores = []\n",
    "\n",
    "\n",
    "for file_name in tqdm(files, desc=\"Processing Files\", unit=\"file\"):\n",
    "    try:\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as test:\n",
    "            file_contents = test.read()\n",
    "\n",
    "        \n",
    "        scores = [\n",
    "            textstat.flesch_reading_ease(file_contents),\n",
    "            textstat.flesch_kincaid_grade(file_contents),\n",
    "            textstat.smog_index(file_contents),\n",
    "            textstat.coleman_liau_index(file_contents),\n",
    "            textstat.dale_chall_readability_score(file_contents)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        readability_scores.append(scores)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "\n",
    "df_readability = pd.DataFrame(readability_scores, columns=[\n",
    "    'Flesch Reading Ease', \n",
    "    'Flesch-Kincaid Grade', \n",
    "    'SMOG Index', \n",
    "    'Coleman-Liau Index', \n",
    "    'Dale-Chall Readability Score'\n",
    "])\n",
    "\n",
    "\n",
    "df_readability['File Name'] = files\n",
    "\n",
    "\n",
    "print(df_readability.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c3cc1-60ef-471d-847e-9086535bf556",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_subset = metadata[[\"flesch\", \"smog\", \"cl\"]] \n",
    "df_subset = df_readability[['Flesch Reading Ease', 'SMOG Index', 'Coleman-Liau Index']]\n",
    "comparison_data = pd.concat([df_subset, metadata_subset], axis=1)\n",
    "comparison_data.describe()\n",
    "\n",
    "#the three common features have a higher mean in the policy-metada file than the averages calculated using the text files. However, the differences\n",
    "#are extremly small (both in absolute and relative terms). These small differneces may be because of the much greater number of measurements inlcuded\n",
    "#in the policy-metadata file (645,125 vs 56,415). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1d09a-79c1-4705-9dfb-887b9eb1fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2(b)\n",
    "folder_path = \"/home3/COMP42315/policy-texts/policies_fromseg_custom/\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "\n",
    "cleaned_texts = []\n",
    "\n",
    "\n",
    "for file_name in tqdm(files, desc=\"Processing Files\", unit=\"file\"):\n",
    "    try:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as test:\n",
    "            file_contents = test.read()\n",
    "\n",
    "\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', file_contents).lower()\n",
    "\n",
    "        \n",
    "        cleaned_texts.append(cleaned_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12391271-a04e-4207-a0f5-a120bfd4ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(cleaned_texts):\n",
    "    df2 = pd.DataFrame()\n",
    "    polarity_values = []\n",
    "    subjectivity_values = []\n",
    "    \n",
    "\n",
    "    for text in tqdm(cleaned_texts, desc=\"Analyzing Sentiment\", unit=\"text\"):\n",
    "        text_blob = TextBlob(text)\n",
    "        polarity_values.append(text_blob.sentiment.polarity) \n",
    "        subjectivity_values.append(text_blob.sentiment.subjectivity)\n",
    "    \n",
    "    df2[\"polarity\"] = polarity_values\n",
    "    df2[\"subjectivity\"] = subjectivity_values\n",
    "    return df2\n",
    "\n",
    "\n",
    "df2 = sentiment_analysis(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b90e2-76f2-4ab3-aab8-a5fb0cf0c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flesch_over_time = metadata['flesch'].groupby(metadata['year']) \n",
    "F = flesch_over_time.describe()\n",
    "F = F.reset_index()\n",
    "x = F[\"year\"]\n",
    "y = F[\"50%\"]\n",
    "plt.scatter(x, y, marker=\"*\", s=10)\n",
    "plt.plot(x, y)\n",
    "#The flesch reading score graph shows that after increasing in the first four years, the average score has decreased over time. The scire for the first\n",
    "# is an outlier and is substantially lower than any of the other scores. Excluding the first measurement, the overall decrease has been from around 38\n",
    "#in 2000 to approximately 32 in 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7207e7-c4b5-4287-b338-f0fa6fd4017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "smog_over_time = metadata['smog'].groupby(metadata['year'])\n",
    "S = smog_over_time.describe().reset_index()\n",
    "x=S[\"year\"]\n",
    "y=S[\"50%\"]\n",
    "plt.scatter(x, y, marker=\"*\", s=10)\n",
    "plt.plot(x, y)\n",
    "#the graph for SMOG index scores shows that after an intial decrease in the first three years, the median score has increased steadily over time.\n",
    "#As with the Flesch Reading graph, the difference between the first two measurements is stark, alathough in the general context the first measurement\n",
    "#is not an outlier. The change overtime (from a low of around 15 to a high of nearly 16.75 in 2020) is smaller than the equivalent for the \n",
    "#Flesch Reading Score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d878abd-4a13-4771-a29c-3336181dd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_over_time = metadata['cl'].groupby(metadata['year'])\n",
    "CL = cl_over_time.describe().reset_index()\n",
    "x=CL[\"year\"]\n",
    "y=CL[\"50%\"]\n",
    "plt.scatter(x, y, marker=\"*\", s=10)\n",
    "plt.plot(x, y)\n",
    "#THe scatter plot for the Coleman-Liau Index shows a gradual upwards trend over tiem. As with both previous graphs the difference between the\n",
    "#first two measurements is stark, with the median for 1996 being an outlier. From 1999, the difference in the median score between years is low. The\n",
    "#Colemian-Liau Index shows the least change over time of the three graph, rising from a low of approximately 12.75 in 1998 to a high of around\n",
    "#13.3 in 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557d97c-82e1-48af-91ed-2c9886281369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polarity boxplot\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.boxplot([df2['polarity'], df2['subjectivity']], \n",
    "            tick_labels=[\"Polarity Scores\", \"Subjectivity Scores\"], \n",
    "            widths=0.2)\n",
    "plt.title('Polarity Boxplot')\n",
    "plt.ylabel('Values')\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "#The graph below presents the boxplots for polarity and subjectivity. Both have a significant number of outlier measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475e766-478a-4cf1-939f-094a69bbe793",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot([df_readability['Dale-Chall Readability Score'], df_readability['Coleman-Liau Index'], df_readability['SMOG Index']], \n",
    "            tick_labels=[\"Dale-Chall Readability Score\", \"Coleman_Liau Index\", \"SMOG Index\"]) \n",
    "plt.axis([None, None, -5, 40])\n",
    "plt.title('Comparison of Various Readability Metrics')\n",
    "plt.ylabel('Values')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#the boxplot below shows the boxplots for the Dale-Chale Readability Score, the Coleman-Liau Index and the SMOG Index. All three have a significant\n",
    "#number of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975bc22-ff55-4431-a0b1-8c05e634b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  \n",
    "plt.boxplot([df_readability['Flesch Reading Ease'], df_readability['Flesch-Kincaid Grade']], \n",
    "            tick_labels=[\"Flesch Reading Ease\", \"Flesch-Kincaid Grade\"], \n",
    "            widths=0.2)\n",
    "plt.title('Comparison of Various Metrics')\n",
    "plt.axis([None, None, -75, 75])\n",
    "plt.ylabel('Values')\n",
    "plt.title('Flesch Reading Ease Boxplot')\n",
    "plt.ylabel('Values')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#The graph below shows the boxplots for the Flesch Reading Ease score and the Flesch_kincaid Grade. Both have a significant number of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900fe487-1cbd-4aff-b13b-15d363f7e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question (c)\n",
    "horizontal_concat_clean = pd.concat([df_readability, df2], axis=1)\n",
    "horizontal_concat.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bb128-c0a7-4789-8f32-7b8f40593a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polarity Values\")\n",
    "for i in range(5):\n",
    "    predictors = horizontal_concat_clean.iloc[:, i]\n",
    "    predictors = pd.to_numeric(predictors, errors='coerce')\n",
    "    polarity = pd.to_numeric(horizontal_concat_clean[\"polarity\"], errors='coerce')\n",
    "    correlation, p_value = scipy.stats.pearsonr(predictors, polarity)\n",
    "    print(f\"Feature {horizontal_concat_clean.columns[i]}: Pearson correlation = {correlation}, p-value = {p_value}\")\n",
    "\n",
    "print(\"Subjectivity Values\")\n",
    "for i in range(5):\n",
    "    predictors = horizontal_concat_clean.iloc[:, i]\n",
    "    predictors = pd.to_numeric(predictors, errors='coerce')\n",
    "    subjectivity = pd.to_numeric(horizontal_concat_clean[\"subjectivity\"], errors='coerce')\n",
    "    correlation, p_value = scipy.stats.pearsonr(predictors, subjectivity)\n",
    "    print(f\"Feature {horizontal_concat_clean.columns[i]}: Pearson correlation = {correlation}, p-value = {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d40f1f-a422-417c-84e5-0f9f40d8de87",
   "metadata": {},
   "source": [
    "The three readability scores contained in the policy-metadata file were: Flesch Reading Ease, SMOG Index and the Coleman-Liau Index. After creating a subset of the original metadata file with only these three measurements, the subset is merged horizontally with a data frame which contains the values calculated using the readability function.\r\n",
    "\r\n",
    "In order to ease the process of calculating subjectivity and polarity for the text files, the function re is used to remove any non-alphabetical characters from the text files. Once this is done the remaining letters are all converted to lower case. The textblob package is then used to calculate each individual polarity and subjectivity score.\r\n",
    "\r\n",
    "Only three of the readability metrics are presented in the graphs showing a change in the median score over time. This is because they are the three that appear in the metadata file and therefore can be analysed by year. The graphs are presented separately as they have different scales.\r\n",
    "Where possible, the boxplots are plotted on the same graph. However, this is not possible for all of the different measures due to differences in scale. Note that for boxplots 2 and 3 the y axis has been limited to exclude a small number of extreme outliers. This is because these outliers stretch the boxplots to an extent where they are almost no longer visible. These outliers can be viewed by removing the plt.axis() command in the code.\r\n",
    "\r\n",
    "Finally, a correlation matrix is plotted to visualise the correlation between the readability metrics and polarity and sensitivity. Using Pearson’s coefficient, two for loops are used to see how many of these correlations are statistically significant. Whilst all correlations are found to be statistically significant, none of the variables show strong correlations with either polarity or sensitivity (the strongest correlation is -0.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4645795-6f56-45d1-b68a-239919899166",
   "metadata": {},
   "source": [
    "# Question 3 (35 marks)\n",
    "\n",
    "For this question, you are asked to perform the task based on the target dataset (drone.csv), which you can download separately on Blackboard Ultra. The target drone dataset contains different features related to the drone physical attributes.  The target variable is 'warning', i.e. warning level. If the warning  level is high then a drone must be grounded immediately to avoid any damage to the drone itself as well as the surroundings (in case of a crash). Design and implement the solution to use data analysis and visualisation for the following tasks: [35%]\n",
    "\n",
    "a) You are required to extract a subset that includes the 'defined_features' and the 'target_variable' (in the subset, there will be 11 features in total including target variable). You are required to extract the 'defined_features' that are as indicated below: \n",
    "\n",
    "defined_features = ['voltage_v', 'voltage_filtered_v', 'current_a', 'current_filtered_a', 'discharged_mah', 'remaining', 'scale', 'load', 'ram_usage'] \n",
    "\n",
    "Perform exploratory data analysis on the drone dataset ('defined_features'), highlight the features that are statistically important and highly correlated to the 'target variable', and visualise them legibly using an appropriate visual method. Save the statistically important features as a subset 'selected_features' and compare them with the 'defined_features'. Highlight any differences and report your findings. [10/35]\n",
    "\n",
    "b) Perform analysis to identify the complex relationship between drone physical attributes ('selected_features') and the warning level ('target_variable') using a probabilistic method. Highlight and visualise the attributes with the highest probabilistic relationship with the target variable. Justify the design choice and showcase the findings using an appropriate visualisation tool. [18/35]\n",
    "\n",
    "c) Perform predictive analysis using at least one machine learning algorithms and validate its performance based on suitable performance metrics. Justify the design choice and showcase the findings using an appropriate visualisation tool. [7/35]\n",
    "\n",
    "[Explain your design and highlight any features in this question's report part of your Jupyter Notebook in no more than 400 words.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a929b-7684-4f3a-abae-85188985f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3(a)\n",
    "df = pd.read_csv(\"drone.csv\")\n",
    "defined_features = df.drop(\"timestamp\",axis=1)\n",
    "defined_features.describe()\n",
    "#display(defined_features)\n",
    "\n",
    "correlation_matrix = defined_features.corr()\n",
    "#display(correlation_matrix)\n",
    "\n",
    "na_counts = df.isna().sum()\n",
    "#print(na_counts)\n",
    "defined_features.dropna()\n",
    "\n",
    "target_variable = df[[\"warning\"]]\n",
    "\n",
    "selected_features_list = []\n",
    "variable = []\n",
    "Correlation = []\n",
    "P_value = []\n",
    "\n",
    "for i in range(9):\n",
    "    feature = defined_features.iloc[:, i]\n",
    "    correlation, p_value = scipy.stats.pearsonr(feature, defined_features[\"warning\"])\n",
    "    print(f\"Feature {defined_features.columns[i]}: Pearson correlation = {correlation}, p-value = {p_value}\")\n",
    "    if p_value < 0.05:\n",
    "        selected_features_list.append(defined_features.columns[i])\n",
    "        variable.append(defined_features.columns[i])\n",
    "        Correlation.append(correlation)\n",
    "        P_value.append(p_value)\n",
    "\n",
    "selected_features = defined_features[selected_features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb358fa-0fea-40f3-b209-45e1ba718179",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, \n",
    "             x_vars=['voltage_v', 'voltage_filtered_v', 'current_a', 'current_filtered_a', \n",
    "                     'remaining', 'discharged_mah', 'scale'],\n",
    "             y_vars=['warning'], \n",
    "             height=2.5) \n",
    "plt.suptitle(\"Pairplot of Defined Features and Target Variable\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#the plots below illustrate the relationship between the selected_features and warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cdb47-ddf3-4d7f-a1ea-bfd71512eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results = pd.DataFrame(list(zip(variable, Correlation, P_value)), columns = [\"Variable\", \"Correlation\", \"p_value\"])\n",
    "correlation_results\n",
    "\n",
    "#a total of 7 variables are included in selected_variables and are listed below. The variables load and ram_usage\n",
    "#are the only two varaibles in the defined_features dataset that are not found to be correlated with warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413dceb-42ec-43c4-b8ee-95164d04eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3(b)\n",
    "#in order to use a bayesian model it is necessary to discretise the variables. #A Bayesian network is chosen to model the probablistic relationship \n",
    "#as it allows for the modelling of complex dependencies between multiple variables whilst accounting for uncertanties and prioir knowledge. \n",
    "\n",
    "voltage_v_labels = ['1. less than 16', '2. Between: 16 and 18', '3. Between: 18 and 20', '4. Greater than: 20']\n",
    "voltage_filtered_v_labels = ['1. less than 16', '2. Between: 16 and 18', '3. Between: 18 and 20', '4. Greater than: 20']\n",
    "current_a_labels = ['1. less than 16', '2. Between: 16 and 18', '3. Between: 18 and 20', '4. Greater than: 20']\n",
    "current_filtered_a_labels = ['1. less than 16', '2. Between: 16 and 18', '3. Between: 18 and 20', '4. Greater than: 20']\n",
    "remaining_labels = [\"1. 0\", \"2. (0, 0.5)\", \"3. >0.5\"]\n",
    "scale_labels = ['2. Less than 1.157143', '1. Exactly 1.157143']\n",
    "warning_labels = ['1. 0', '2. 1', '3. 2', '4. 3']\n",
    "\n",
    "target_variable = pd.DataFrame(defined_features.warning)\n",
    "target_variable['warning'] = pd.cut(target_variable['warning'], bins=4, labels=warning_labels, precision=2)\n",
    "for column in target_variable:\n",
    "    print(target_variable.groupby(column)[column].count().reset_index(name='Count').to_dict(orient='records'))\n",
    "\n",
    "def make_discrete(df):\n",
    "\n",
    "    discrete_df = pd.DataFrame()\n",
    "\n",
    "    discrete_df['voltage_v'] = pd.cut(df['voltage_v'], bins=4, labels=voltage_v_labels, precision=2)\n",
    "    discrete_df['voltage_filtered_v'] = pd.cut(df['voltage_filtered_v'], bins=4, labels=voltage_filtered_v_labels, precision=2)\n",
    "    discrete_df['current_a'] = pd.cut(df['current_a'], bins=4, labels=current_a_labels, precision=2)\n",
    "    discrete_df['current_filtered_a'] = pd.cut(df['current_filtered_a'], bins=4, labels=current_filtered_a_labels, precision=2)\n",
    "    discrete_df['remaining'] = pd.cut(df['remaining'], bins=3, labels=remaining_labels, precision=2)\n",
    "    discrete_df['scale'] = pd.cut(df['scale'], bins=2, labels=scale_labels, precision=2)\n",
    "    discrete_df = discrete_df.astype('object')\n",
    "    \n",
    "    return discrete_df\n",
    "\n",
    "discrete_df = make_discrete(selected_features) \n",
    "discrete_df = pd.concat([discrete_df, target_variable], axis=1)\n",
    "for column in discrete_df:\n",
    "    discrete_df.groupby(column)[column].count().reset_index(name='Count').to_dict(orient='records')\n",
    "\n",
    "#splitting data into training and testing sets. \n",
    "training_data, testing_data = train_test_split(discrete_df, test_size=0.2, random_state=20)\n",
    "\n",
    "#fit Bayesian Network\n",
    "hc = HillClimbSearch(data=training_data)\n",
    "estimate = hc.estimate(scoring_method='k2score')\n",
    "model = BayesianNetwork(estimate)\n",
    "\n",
    "model.cpds = []\n",
    "\n",
    "model.fit(data=training_data,\n",
    "    estimator=BayesianEstimator,\n",
    "    prior_type='BDeu',\n",
    "    equivalent_sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5434462-dec7-452f-9fa7-0b7b45010ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(14,10))\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(model.edges)\n",
    "\n",
    "\n",
    "G.add_nodes_from(model.nodes)\n",
    "pos = nx.circular_layout(G)\n",
    "DAG = G.to_directed()\n",
    "nx.topological_sort(DAG)\n",
    "\n",
    "nx.draw_networkx(G,\n",
    "                pos=pos,\n",
    "                with_labels=True,\n",
    "                node_size=[2000] * len(G.nodes),\n",
    "                arrowsize=30,\n",
    "                alpha=0.7,\n",
    "                font_weight=\"bold\",\n",
    "                width=2.0) \n",
    "\n",
    "tt_g = G.subgraph(nodes=['resourceState'])\n",
    "nx.draw(tt_g, pos=pos, with_labels=False, arrowsize=0, node_size=4100, alpha=0.7, font_weight=\"bold\", node_color='#063970')\n",
    "\n",
    "plt.show()\n",
    "#graph below shows clearly the different conditional relationships amongst all the variables selected. The variables\n",
    "#with the highest probablistic relationship with the target variable are scale, remaining, voltage_filtered_v and \n",
    "#current_filtered_a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886db4d3-9c4a-4a74-a5e8-864f75bb503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_to_include = ['scale', 'remaining', 'voltage_filtered_v', 'current_filtered_a', 'warning']  \n",
    "\n",
    "subgraph = G.subgraph(nodes=nodes_to_include)\n",
    "pos = nx.circular_layout(subgraph)  # You can also experiment with other layouts (e.g., spring_layout)\n",
    "nx.draw_networkx(subgraph,\n",
    "                 pos=pos,\n",
    "                 with_labels=True,\n",
    "                 node_size=[2000] * len(subgraph.nodes),\n",
    "                 arrowsize=30,\n",
    "                 alpha=0.7,\n",
    "                 font_weight=\"bold\",\n",
    "                 width=2.0)\n",
    "plt.show()\n",
    "#network below only shows those nodes which have the highest probablistic relationship with warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4bf22-aa9d-4932-a7c9-74de92ccf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determining accuracy, precision and F1 score for each variable in the bayesian network\n",
    "accuracy_dict = {}\n",
    "f1_dict = {}\n",
    "precision_dict = {}\n",
    "\n",
    "for column in testing_data:\n",
    "    predict_data = testing_data.copy()\n",
    "    predict_data.drop(column, axis=1, inplace=True)\n",
    "    y_pred = model.predict(predict_data)\n",
    "    accuracy = accuracy_score(testing_data[column], y_pred)\n",
    "    print(f'{column} Accuracy score: {accuracy}')\n",
    "    accuracy_dict[column] = accuracy\n",
    "    f1 = f1_score(testing_data[column], y_pred, average='weighted')  # 'weighted' takes class imbalance into account\n",
    "    print(f'{column} F1 score: {f1}')\n",
    "    f1_dict[column] = f1\n",
    "    precision = precision_score(testing_data[column], y_pred, average='weighted')  # 'weighted' for class imbalance\n",
    "    print(f'{column} Precision score: {precision}')\n",
    "    precision_dict[column] = precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62633d2-7450-4eaa-b983-0fc864e9a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3(c)\n",
    "\n",
    "#Binomial logistic regression is used. This classification method is easier to interpret, train and implement than\n",
    "#a linear regression model. In the case of warning where the principal concern is distinguishing between 0 and 3 \n",
    "#logistic regression is a suitable method of anlaysis. \n",
    "df = df[df[\"warning\"].isin([0, 3])]\n",
    "y = df[\"warning\"]\n",
    "X = df[[\"scale\", \"voltage_filtered_v\", \"remaining\", \"current_filtered_a\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "reg = linear_model.LogisticRegression(max_iter=10000, random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "#The results of the logistic regression are visualised through the use of a confusion matrix which shows the \n",
    "#distribution of the model's predictions, and whether they were correct. \n",
    "y_pred = reg.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=reg.classes_)\n",
    "cm_display.plot(cmap='Blues', values_format='d', ax=None)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Logistic Regression model accuracy: {metrics.accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"r-squared score: {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e44e35-5aa2-4cca-be5b-3e7b08d5470f",
   "metadata": {},
   "source": [
    "The first part of the answer deals with finding which variables are correlated with the target variable. The method above uses pearson's r to extract the statistically significant variables from defined_features. The for loop finds the statistically significant variables and then appends them to an empty list. This list is then used to create the selected_features subset.\n",
    "\n",
    "The selected features subset is created using <if p_value < 0.05> statement. The results show that 7 variables are correlated with warning, with only load and ram_usage not found to be correlated. When conducting statistical analysis, the discharged_mah value is excluded. This is because its correlation is far weaker than the other variables and is therefore unlikely to significantly improve any future model. This weaker correlation is visible in the pairplots generated. This approach of excluding discharged_mah is vindicated by the high accuracy scores found at the end of the answer.\n",
    "\n",
    "A Bayesian network is then used to analyse the conditional dependencies between the different variables. The model is trained on 80% of the data from the discretised data frame. During discretisation each variable was broken down into multiple categories, with the cut-off points chosen to best reflect the structure of each variable. The fitted Bayesian model is then evaluated using the K2 score. The model's accuracy scores were high across the board, with only the accuracy score for voltage_v falling below 90%.\n",
    "\n",
    "When conducting the logistic regression, the model is limited to predicting whether warning will equal 0 or 3, excluding levels 1 and 2. This is because the data is not uniformly distributed; levels 0 and 3 contain over 99% of all the measurements. The main aim of the model is therefore to distinguish between the two main categories, which it does with a high level of accuracy and precision. The predictor variables selected are those that the Bayesian network identified as having a conditional probability relationship with warning.\n",
    "\n",
    "When analysing the statistics printed the model does exceptionally well across the board, with 99% of the model's predictions being correct. The results of the classification report show that the model does slightly better in predicting warning level 3 compared to level 0 although the difference is minor. The r-squared score of nearly 90%, meaning that the model's fit is extremely high. This validates the decision to only include the 4 predictor variables chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
